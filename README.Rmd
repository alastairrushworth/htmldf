---
output: github_document
---


# htmldf

The package `htmldf` contains a single function `html_df()` which accepts a vector of urls as an input and from each will attempt to download each page, extract and parse the html.  The result is returned as a `tibble` where each row corresponds to a document, and the columns contain page attributes and metadata extracted from the html, including:

+ page title
+ inferred language
+ RSS feeds
+ image links
+ twitter, github and linkedin profiles
+ page size, generator and server
+ page accessed date
+ page published or last updated dates


Installation and usage
---  

To install the package:

```{r, eval=FALSE}
remotes::install_github('alastairrushworth/htmldf)
```

To use `html_df`
```{r}
library(htmldf)
urlx <- c("https://ropensci.org/blog/2020/02/21/ropensci-leadership/",
          "https://es.wikipedia.org/wiki/Wikipedia_en_espa%C3%B1ol", 
          "https://juliasilge.com/")
z <- html_df(urlx, show_progress = FALSE)
z
```

Page titles
```{r}
z$title
```

RSS feeds
```{r}
z$rss
```

Social profiles
```{r}
z$social
```




Comments? Suggestions? Issues?
---  

Any feedback is welcome! Feel free to write a github issue or send me a message on [twitter](https://twitter.com/rushworth_a).




